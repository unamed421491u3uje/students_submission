{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52gKR3AgiY46"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "from transformers import AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d7iJa4Gf_vX-",
        "outputId": "0bc5dcd2-b060-455b-82fa-f2b7d41c67db"
      },
      "outputs": [],
      "source": [
        "LABEL_PAD = 2\n",
        "\n",
        "def qqplot_points(data, dist, *params):\n",
        "    percs = np.linspace(0, 100, 101)\n",
        "    qn_sample = np.percentile(data, percs)\n",
        "    qn_dist = dist.ppf(percs / 100.0, *params)\n",
        "    return qn_sample, qn_dist\n",
        "\n",
        "def plot_histogram(ax, data, bins, absmax_x, name='Layer'):\n",
        "    ax.hist(data, bins=bins, density=True, alpha=0.6, color='g')\n",
        "    ax.set_xlim(-absmax_x, absmax_x)\n",
        "    ax.set_title(f'{name} - Histogram with Distribution Fits')\n",
        "    ax.set_xlabel('Value', labelpad=LABEL_PAD)\n",
        "    ax.set_ylabel('Density', labelpad=LABEL_PAD)\n",
        "    ax.grid()\n",
        "\n",
        "def plot_distribution_fit(ax, x, data, dist, color, label, subset_ratio, std_factor=1):\n",
        "    if subset_ratio is not None and subset_ratio < 1.0:\n",
        "        subset_size = int(len(data) * subset_ratio)\n",
        "        data_subset = data[:subset_size]\n",
        "    else:\n",
        "        data_subset = data\n",
        "\n",
        "    params = dist.fit(data_subset)\n",
        "    # Adjust the std parameter for the Gaussian distribution\n",
        "    if dist == stats.norm:\n",
        "        params = (params[0], params[1] * std_factor)\n",
        "    pdf = dist.pdf(x, *params)\n",
        "    ax.plot(x, pdf, color=color, linewidth=2, label=label)\n",
        "    return params\n",
        "\n",
        "def plot_qq(ax, data, dist, params, color, label, xy_lim, line_color='r', std_factor=1, name='Layer'):\n",
        "    # Adjust the std parameter for the Gaussian distribution\n",
        "    if dist == stats.norm:\n",
        "        params = (params[0], params[1] * std_factor)\n",
        "    qn_sample, qn_dist = qqplot_points(data, dist, *params)\n",
        "    ax.plot(qn_dist, qn_sample, 'o', color=color, label=label, markersize=5)\n",
        "    ax.plot(qn_dist, qn_dist, color=line_color)\n",
        "    ax.set_title(f'{name} - QQ Plot')\n",
        "    ax.set_xlabel('Theoretical Quantiles', labelpad=LABEL_PAD)\n",
        "    ax.set_ylabel('Sample Quantiles', labelpad=LABEL_PAD)\n",
        "    ax.set_xlim([-xy_lim, xy_lim])\n",
        "    ax.set_ylim([-xy_lim, xy_lim])\n",
        "    ax.grid()\n",
        "\n",
        "def plot_distributions(values_dict, bins=150, xlim_percentage=0.5, subset_ratio=None,\n",
        "                       std_factors=[1], save_path='profiled_values.png', show_params=True):\n",
        "    num_layers = len(values_dict)\n",
        "    fig = plt.figure(figsize=(6, 5 * num_layers))\n",
        "    gs = GridSpec(num_layers, 2, figure=fig)\n",
        "    colors = sns.color_palette('muted')\n",
        "\n",
        "    for i, (name, value) in enumerate(values_dict.items()):\n",
        "        value = value.flatten()\n",
        "        value = value.to(torch.float32)\n",
        "        \n",
        "        absmax_x = np.abs(value).max() * xlim_percentage\n",
        "\n",
        "        # Histogram and distribution fits\n",
        "        ax_hist = fig.add_subplot(gs[i, 0])\n",
        "        plot_histogram(ax_hist, value, bins, absmax_x, name=name)\n",
        "\n",
        "        x = np.linspace(-absmax_x, absmax_x, 1000)\n",
        "\n",
        "        # Combined QQ plot\n",
        "        ax_qq = fig.add_subplot(gs[i, 1])\n",
        "        \n",
        "        # Fit and plot Gaussian distribution\n",
        "        gaussian_params_list = []\n",
        "        for idx, std_factor in enumerate(std_factors):\n",
        "            color = colors[idx % len(colors)]  # Cycle through colors\n",
        "            gaussian_params = plot_distribution_fit(ax_hist, x, value, stats.norm, color, f'Normal ({std_factor} $\\\\times\\\\,\\\\sigma$)', subset_ratio, std_factor)\n",
        "            plot_qq(ax_qq, value, stats.norm, gaussian_params, color, f'Normal ({std_factor} $\\\\times\\\\,\\\\sigma$)', xy_lim=absmax_x, std_factor=std_factor)\n",
        "            gaussian_params_list.append(gaussian_params)\n",
        "\n",
        "        # Fit and plot Student's t distribution\n",
        "        student_params = plot_distribution_fit(ax_hist, x, value, stats.t, 'g', \"Student's t\", subset_ratio)\n",
        "        plot_qq(ax_qq, value, stats.t, student_params, 'g', \"Student's t\", xy_lim=absmax_x, name=name)\n",
        "\n",
        "        # Add text for parameters\n",
        "        if show_params:\n",
        "            text_str = \"Params:\\n\"\n",
        "            for idx, std_factor in enumerate(std_factors):\n",
        "                text_str += f\"Normal: (std x {std_factor}): mean={gaussian_params_list[idx][0]:.2f}, std={gaussian_params_list[idx][1]:.2f}\\n\"\n",
        "            text_str += f\"Student's t: df={student_params[0]:.2f}, loc={student_params[1]:.2f}, scale={student_params[2]:.2f}\"\n",
        "            ax_qq.text(0.95, 0.05, text_str, transform=ax_qq.transAxes, fontsize=10, verticalalignment='bottom', horizontalalignment='right')\n",
        "\n",
        "        ax_hist.set_title(f'Weight Distribution')\n",
        "        ax_qq.set_title(f'QQ Plot')\n",
        "        ax_hist.legend()\n",
        "        ax_qq.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.95, wspace=0.34)\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_distribution_fits(values_dict, subset_ratio=None):\n",
        "    fit_dict = {}  # Dictionary to store fit results for each layer\n",
        "\n",
        "    for name, value in values_dict.items():\n",
        "        value = value.flatten()\n",
        "        value = value.to(torch.float32)\n",
        "\n",
        "        # Fit Normal and Student's t distributions\n",
        "        norm_params = fit_distribution(value, stats.norm, subset_ratio)\n",
        "        student_params = fit_distribution(value, stats.t, subset_ratio)\n",
        "\n",
        "        # Perform KS test for both distributions\n",
        "        ks_stat_norm, _ = stats.kstest(value, 'norm', norm_params)\n",
        "        ks_stat_student, _ = stats.kstest(value, 't', student_params)\n",
        "\n",
        "        dof_student = min(student_params[0], 15)\n",
        "\n",
        "        # Store the results in the dictionary\n",
        "        fit_dict[name] = {\n",
        "            'norm_params': norm_params,\n",
        "            'student_params': student_params,\n",
        "            'dof_student': dof_student,\n",
        "            'ks_norm': ks_stat_norm,\n",
        "            'ks_student': ks_stat_student\n",
        "        }\n",
        "\n",
        "    return fit_dict\n",
        "\n",
        "def aggregate_and_analyze(fit_dict):\n",
        "    # Initialize dictionaries to hold values for each group\n",
        "    grouped_dofs = {}\n",
        "    grouped_ks_norm = {}\n",
        "    grouped_ks_student = {}\n",
        "\n",
        "    # Grouping the values\n",
        "    for layer_name, stats in fit_dict.items():\n",
        "        # Extract the last part of the layer name\n",
        "        layer_type = layer_name.split('.')[-1]\n",
        "\n",
        "        # Aggregate degrees of freedom for Student's t-distribution and KS test statistics\n",
        "        dof = stats['dof_student']\n",
        "        grouped_dofs.setdefault(layer_type, []).append(dof)\n",
        "        grouped_ks_norm.setdefault(layer_type, []).append(stats['ks_norm'])\n",
        "        grouped_ks_student.setdefault(layer_type, []).append(stats['ks_student'])\n",
        "\n",
        "    # Calculating mean and standard deviation for each group\n",
        "    aggregated_stats = {}\n",
        "    for layer_type in grouped_dofs:\n",
        "        mean_dof = np.mean(grouped_dofs[layer_type])\n",
        "        std_dof = np.std(grouped_dofs[layer_type])\n",
        "        mean_ks_norm = np.mean(grouped_ks_norm[layer_type])\n",
        "        mean_ks_student = np.mean(grouped_ks_student[layer_type])\n",
        "\n",
        "        aggregated_stats[layer_type] = {\n",
        "            'mean_dof': mean_dof, 'std_dof': std_dof,\n",
        "            'mean_ks_norm': mean_ks_norm, 'mean_ks_student': mean_ks_student\n",
        "        }\n",
        "\n",
        "    # Calculate overall mean and std for degrees of freedom and KS test statistics\n",
        "    all_dofs = [stat['dof_student'] for stat in fit_dict.values()]\n",
        "    all_ks_norm = [stat['ks_norm'] for stat in fit_dict.values()]\n",
        "    all_ks_student = [stat['ks_student'] for stat in fit_dict.values()]\n",
        "\n",
        "    total_mean_dof = np.mean(all_dofs)\n",
        "    total_std_dof = np.std(all_dofs)\n",
        "    total_mean_ks_norm = np.mean(all_ks_norm)\n",
        "    total_mean_ks_student = np.mean(all_ks_student)\n",
        "\n",
        "    # Add a \"total\" group to the aggregated statistics\n",
        "    aggregated_stats['total'] = {\n",
        "        'mean_dof': total_mean_dof, 'std_dof': total_std_dof,\n",
        "        'mean_ks_norm': total_mean_ks_norm, 'mean_ks_student': total_mean_ks_student\n",
        "    }\n",
        "\n",
        "    return aggregated_stats\n",
        "\n",
        "\n",
        "def fit_distribution(data, distribution, subset_ratio=None):\n",
        "    if subset_ratio is not None and subset_ratio < 1.0:\n",
        "        subset_size = int(len(data) * subset_ratio)\n",
        "        data = data[:subset_size]  # Slicing the data array to get the subset\n",
        "    \n",
        "    params = distribution.fit(data)\n",
        "    return params\n",
        "\n",
        "def print_dict_in_lines(d, indent=0):\n",
        "    for key, value in d.items():\n",
        "        if isinstance(value, dict):\n",
        "            print(' ' * indent + f\"{key}:\")\n",
        "            print_dict_in_lines(value, indent + 2)\n",
        "        else:\n",
        "            print(' ' * indent + f\"{key} -> {value:}\")\n",
        "\n",
        "def format_values_single_string(s: str):\n",
        "    import re\n",
        "\n",
        "    # Extracting the values\n",
        "    mean_dof = float(re.search(r\"mean_dof -> ([\\d.]+)\", s).group(1))\n",
        "    std_dof = float(re.search(r\"std_dof -> ([\\d.]+)\", s).group(1))\n",
        "    mean_ks_norm = float(re.search(r\"mean_ks_norm -> ([\\d.]+)\", s).group(1))\n",
        "    mean_ks_student = float(re.search(r\"mean_ks_student -> ([\\d.]+)\", s).group(1))\n",
        "\n",
        "    # Calculating the difference between mean_ks_norm and mean_ks_student\n",
        "    ks_difference = mean_ks_norm - mean_ks_student\n",
        "\n",
        "    # Formatting into a single string\n",
        "    single_string_output = f\"{mean_dof:.2f}\\(_{{{std_dof:.2f}}}\\) & {ks_difference:.3f}\"\n",
        "\n",
        "    return single_string_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'facebook/opt-125m'\n",
        "\n",
        "# model = timm.create_model(model_name, pretrained=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", trust_remote_code=True)\n",
        "# model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "# model = models.resnet18(pretrained=True)\n",
        "module_types = (torch.nn.Linear, torch.nn.Conv1d, torch.nn.Conv2d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only analyze a subset of layers for testing\n",
        "MAX_LAYERS = 1\n",
        "# Subset ratio for reducing the number of samples for testing\n",
        "SUBSET_RATIO = 0.1\n",
        "\n",
        "weights = {name: module.weight.data for name, module in model.named_modules() if isinstance(module, module_types) and \\\n",
        "                                                                                            'embed' not in name and 'layer_norm' not in name}\n",
        "first_layers = dict(list(weights.items())[0:MAX_LAYERS])\n",
        "print(first_layers.keys())\n",
        "\n",
        "plot_distributions(first_layers, bins=100, subset_ratio=SUBSET_RATIO, std_factors=[1.0], xlim_percentage=0.2, show_params=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weight_raw_dict = calculate_distribution_fits(first_layers, subset_ratio=0.1)\n",
        "print_dict_in_lines(weight_raw_dict)\n",
        "\n",
        "# Analyze the DOF dictionary\n",
        "aggregated_stats = aggregate_and_analyze(weight_raw_dict)\n",
        "\n",
        "# Usage with your aggregated_stats dictionary\n",
        "print(\"Aggregated Stats:\")\n",
        "print_dict_in_lines(aggregated_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "activation_list = []\n",
        "activation_names = []\n",
        "counter = 0\n",
        "\n",
        "MAX_LAYERS = 10\n",
        "# Subset ratio for reducing the number of samples for testing\n",
        "SUBSET_RATIO = 0.1\n",
        "INPUT_LENGTH = 64\n",
        "FILTERS = []\n",
        "\n",
        "def collect_activations(module, input, output):\n",
        "    output = output.to(torch.float32)\n",
        "    activation_list.append(output.detach().cpu())\n",
        "\n",
        "# Attaching hooks with a break after attaching to the first N modules of interest\n",
        "# and skipping modules based on filters\n",
        "for name, module in model.named_modules():\n",
        "    # Check if current module's name contains any filter term\n",
        "    if isinstance(module, module_types) and not any(filter_term in name for filter_term in FILTERS):\n",
        "        module.register_forward_hook(lambda m, i, o: collect_activations(m, i, o))\n",
        "        activation_names.append(name)  # 'name' includes the full path\n",
        "        counter += 1  # Increment the counter\n",
        "        if counter >= MAX_LAYERS:  # Check if the counter has reached the limit\n",
        "            break  # Break out of the loop\n",
        "\n",
        "# Assuming a vocabulary size of 1024 for the language model\n",
        "input_data = torch.randint(low=0, high=1024, size=(1, INPUT_LENGTH))\n",
        "\n",
        "# Forward pass with input data to collect activations\n",
        "with torch.no_grad():\n",
        "    model(input_data)\n",
        "\n",
        "# dict from names and list\n",
        "activation_dict = dict(zip(activation_names, activation_list))\n",
        "\n",
        "# Now you can use the plot_weight_distributions function on activation_list\n",
        "act_dist_dict = calculate_distribution_fits(activation_dict, subset_ratio=SUBSET_RATIO)\n",
        "print_dict_in_lines(act_dist_dict)\n",
        "\n",
        "# Analayze the DOF dictionary\n",
        "aggregated_stats = aggregate_and_analyze(act_dist_dict)\n",
        "\n",
        "# Usage with your aggregated_stats dictionary\n",
        "print(\"Aggregated Stats:\")\n",
        "print_dict_in_lines(aggregated_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the range of degrees of freedom\n",
        "dfs = [1, 2, 3, 4, 5, 10, 100]\n",
        "\n",
        "# Create a range of x values for plotting\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "\n",
        "# Plot t distributions for each degree of freedom\n",
        "plt.figure(figsize=(6, 4))\n",
        "for df in dfs:\n",
        "    # Calculate the t distribution\n",
        "    rv = stats.t(df, loc=0, scale=1)\n",
        "    plt.plot(x, rv.pdf(x), label=r\"$\\nu={}$\".format(df))  # LaTeX for ν=df\n",
        "\n",
        "plt.title('Student\\'s t-Distributions')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Probability Density')\n",
        "# Create legend with a title\n",
        "legend = plt.legend(title='Degrees \\nof Freedom')\n",
        "# Center-align the title of the legend\n",
        "legend.get_title().set_ha(\"center\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"t-distributions\", dpi=300)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
